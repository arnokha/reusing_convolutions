{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that reuse gets the same (or close) activations as base method\n",
    " - Check by doing the reuse method in forward, then redo using base method, then check equality\n",
    " - Use CPU, does not fit into CUDA memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import time\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "options = {\"game\":\"YarsRevenge-v0\",            ## Game to train policy on\n",
    "           \"use_all_channels\":False,    ## Use 3 channels (RGB) or 1(Greyscale) (MUST UPDATE PREPRO TO ALLOW FOR RGB)\n",
    "           \"save_model\":False,          ## Save model after training (NYI)\n",
    "           \"n_conv_layers\":2,           ## Number of layers to use (as of now, only 2 supported)\n",
    "           \"n_channels_out_1\":20,       ## Number of channels/filters in conv1\n",
    "           \"n_channels_out_2\":40,       ## Number of channels/filters in conv1\n",
    "           \"lr\":0.0005,                   ## Learning rate for training\n",
    "           \"batch_size\":1,              ## Update policy ever x episodes\n",
    "           \"n_episodes\":10,             ## Number of episodes to train for\n",
    "           \"gamma\":0.99,                ## Discount factor for reward\n",
    "           \"n_reset_graph\":5,           ## Number of episodes to carry computational graph for before resetting\n",
    "          # \"device\":\"cpu\",              ## Always use CPU\n",
    "           \"kernel_size\":3,             ## No support for varying kernel sizes yet\n",
    "           \"render\":False               ## Render gameplay\n",
    "          }\n",
    "options[\"n_channels_in\"] = 3 if options[\"use_all_channels\"] else 1  ## Number of input (color) channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper to turn observations into torch tensor, and read in observation info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Observation -> torch tensor\n",
    "def ob2torch(observation):\n",
    "    return torch.tensor(observation.reshape(1, \n",
    "                                            3, # options[\"n_channels_in\"], \n",
    "                                            n_dim_x, \n",
    "                                            n_dim_y)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(options[\"game\"])\n",
    "observation = env.reset()\n",
    "print(observation.shape)\n",
    "observations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_dim_x_d2 = observation.shape[0] // 2\n",
    "n_dim_y_d2 = observation.shape[1] // 2\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    return torch.tensor(I.reshape(1, 1, n_dim_x_d2, n_dim_y_d2)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "n_channels = 3\n",
    "\n",
    "n_dim_x = prepro(observation).shape[2]\n",
    "n_dim_y = prepro(observation).shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Base: Conv1 -> Conv2 -> FC -> Softmax -> Action Probabilities\n",
    "class TEST_Reuse_Policy(nn.Module):\n",
    "    def __init__(self, n_out_channels_1=options[\"n_channels_out_1\"], \n",
    "                       n_out_channels_2=options[\"n_channels_out_2\"],\n",
    "                       n_channels_in=options[\"n_channels_in\"],\n",
    "                       kernel_size=options[\"kernel_size\"]):\n",
    "        super(TEST_Reuse_Policy, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.x_prev = None\n",
    "        \n",
    "        ## Layers\n",
    "        self.conv1 = nn.Conv2d(n_channels_in, n_out_channels_1, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv2d(n_out_channels_1, n_out_channels_2, kernel_size=kernel_size)\n",
    "        self.fc = nn.Linear(n_out_channels_2 * (n_dim_x - 2*(kernel_size-1)) *(n_dim_y - 2*(kernel_size-1)), \n",
    "                            env.action_space.n, \n",
    "                            bias=False) ## Assumes kernel_size=3\n",
    "        \n",
    "        self.last_c1 = None  ## Last activation of conv1 layer\n",
    "        self.last_c2 = None  ## Last activation of conv2 layer\n",
    "        \n",
    "        ## Training\n",
    "        #self.policy_history = Variable(torch.Tensor()).to(device=options[\"device\"]) \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.reward_history = []         # Overall reward and loss history\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## Get difference of frames\n",
    "        if self.x_prev is not None:\n",
    "            x_diff = (self.x_prev - x)\n",
    "            self.x_prev = x\n",
    "            \n",
    "        ## Process for the first time\n",
    "        else:\n",
    "            out = F.relu(self.conv1(x))\n",
    "            self.last_c1= out.clone()\n",
    "            out = F.relu(self.conv2(out))\n",
    "            self.last_c2= out.clone()\n",
    "            out = out.view(-1)\n",
    "            out = self.fc(out)\n",
    "            out = torch.softmax(out, dim=-1)\n",
    "            self.x_prev = x\n",
    "            return out\n",
    "            \n",
    "        ## Get indices to redo\n",
    "        redo_idx = x_diff.nonzero()\n",
    "        if redo_idx.nelement() == 0:\n",
    "            out = self.last_c2.view(-1)\n",
    "            out = self.fc(out)\n",
    "            return torch.softmax(out, dim=-1)\n",
    "\n",
    "        ## Get min/max of indices to redo, accounting for size of convolution and borders\n",
    "        min_idx_x = redo_idx.min(-2)[0][2].item()\n",
    "        min_idx_y = redo_idx.min(-2)[0][3].item()\n",
    "        max_idx_x = redo_idx.max(-2)[0][2].item()\n",
    "        max_idx_y = redo_idx.max(-2)[0][3].item()\n",
    "        r_x1 = max(min_idx_x - (self.kernel_size - 1), 0)\n",
    "        r_x2 = min(max_idx_x + self.kernel_size, n_dim_x)\n",
    "        r_y1 = max(min_idx_y - (self.kernel_size - 1), 0)\n",
    "        r_y2 = min(max_idx_y + self.kernel_size, n_dim_y)\n",
    "        \n",
    "        ## Redo first level of convolutions, assign result to the proper area of the previous activations\n",
    "        redo_area = x[:,:,r_x1:r_x2,r_y1:r_y2]\n",
    "        redo_area = self.conv1(redo_area)\n",
    "        redo_area = F.relu(redo_area)\n",
    "        c1 = self.last_c1.clone()\n",
    "        c1[:,:,r_x1:r_x1+redo_area.shape[2],r_y1:r_y1+redo_area.shape[3]] = redo_area\n",
    "        \n",
    "        ## Get min/max of indices to redo for 2nd conv, accounting for size of convolution and borders\n",
    "        r_x1 = max(r_x1 - (self.kernel_size - 1), 0)\n",
    "        r_x2 = min(r_x2 + self.kernel_size, n_dim_x)\n",
    "        r_y1 = max(r_y1 - (self.kernel_size - 1), 0)\n",
    "        r_y2 = min(r_y2 + self.kernel_size, n_dim_y)\n",
    "        \n",
    "        ## Redo 2nd layer of convolutions, assign it to the proper area of the previous activations\n",
    "        redo_area = F.relu(self.conv2(c1[:,:,r_x1:r_x2,r_y1:r_y2]))\n",
    "        c2 = self.last_c2.clone()\n",
    "        c2[:,:,r_x1:r_x1+redo_area.shape[2],r_y1:r_y1+redo_area.shape[3]] = redo_area\n",
    "        \n",
    "        out = c2.view(-1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        self.last_c1 = c1\n",
    "        self.last_c2 = c2\n",
    "        \n",
    "        \n",
    "        ## Redo the convolution the base way, check for discrepency\n",
    "        out2 = F.relu(self.conv1(x))\n",
    "        out2 = F.relu(self.conv2(out2))\n",
    "        out2 = out2.view(-1)\n",
    "        out2 = self.fc(out2)\n",
    "        assert(torch.allclose(out,out2,rtol=1e-4,atol=1e-4))\n",
    "        \n",
    "        \n",
    "        return torch.softmax(out, dim=-1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "def select_action(state, policy):\n",
    "    state = state.to(device=device)\n",
    "    #state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action).to(device=device))\n",
    "    return action.item()\n",
    "\n",
    "def update_policy(policy, optimizer, retain_graph=False):\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + options[\"gamma\"] * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns).to(device=device)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 0.0001)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    #print(policy_loss)\n",
    "    #policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss = sum(policy_loss)\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 592379725]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "env = gym.make(options[\"game\"])\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward for this ep(0): 4140.00\n",
      "Total reward for this ep(1): 1449.00\n",
      "Total reward for this ep(2): 966.00\n",
      "Total reward for this ep(3): 2139.00\n",
      "Total reward for this ep(4): 2691.00\n",
      "Total reward for this ep(5): 5544.00\n",
      "Total reward for this ep(6): 5244.00\n",
      "Total reward for this ep(7): 4899.00\n",
      "Total reward for this ep(8): 5137.00\n",
      "Total reward for this ep(9): 5582.00\n",
      "270.62579560279846\n"
     ]
    }
   ],
   "source": [
    "policy = TEST_Reuse_Policy()\n",
    "policy = policy.to(device=options[\"device\"])\n",
    "optimizer = optim.Adam(policy.parameters(), lr=options[\"lr\"])\n",
    "\n",
    "episode_number = 0\n",
    "reward_sum=0\n",
    "observation = env.reset()\n",
    "policy.x_prev=None\n",
    "\n",
    "batch_size = 1\n",
    "x_prev = torch.zeros((1,1,n_dim_x_d2,n_dim_y_d2))\n",
    "\n",
    "start=time.time()\n",
    "while(episode_number < 10):\n",
    "    curr_img = prepro(observation)\n",
    "    x = curr_img.reshape(1,1,n_dim_x_d2,n_dim_y_d2)\n",
    "    \n",
    "    # forward the policy network and sample an action from the returned probability\n",
    "    action = select_action(torch.tensor(x - x_prev).float(), policy)\n",
    "    \n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    policy.rewards.append(reward)\n",
    "    reward_sum += reward\n",
    "    x_prev = x\n",
    "    \n",
    "    if done: # an episode finished\n",
    "        print(\"Total reward for this ep({0:d}): {1:.2f}\".format(episode_number, reward_sum))\n",
    "        episode_number += 1\n",
    "\n",
    "        if episode_number % batch_size == 0:\n",
    "            update_policy(policy, optimizer, retain_graph=False)\n",
    "            policy.x_prev=None\n",
    "            #if episode_number % options[\"n_reset_graph\"] != 0 :\n",
    "            #    update_policy(policy, optimizer, retain_graph=True)\n",
    "            #else:\n",
    "            #    update_policy(policy, optimizer, retain_graph=False)\n",
    "            #    policy.x_prev=None\n",
    "        \n",
    "        if episode_number % 50 == 0:\n",
    "            PATH = 'models/reuse_fix'\n",
    "            torch.save({\n",
    "                'episode_number': episode_number,\n",
    "                'model_state_dict': policy.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, PATH)\n",
    "            \n",
    "        observation = env.reset()\n",
    "        reward_sum=0\n",
    "        \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
