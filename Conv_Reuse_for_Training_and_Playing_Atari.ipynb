{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both Training and Inference are working (runnable)\n",
    "\n",
    "#### Credit:\n",
    "[Andrej Karpathy](http://karpathy.github.io/2016/05/31/rl/) for main training loop  \n",
    "[Pytorch contributors](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py) REINFORCE functions select_action() and updade_policy()/finish_episode()  \n",
    "\n",
    "#### Notes\n",
    " - On CUDA, Reuse tends to be a little slower than Base because of overhead\n",
    "   - Can use profiler (torch.autograd.profiler) to verify\n",
    "   - However, Reuse on the CPU gets close to the GPU time for the base network for some games\n",
    " - Running all cells in succession may result in out of memory error for CUDA.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import time\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "options = {\"game\":\"YarsRevenge-v0\",            ## Game to train policy on\n",
    "           \"use_all_channels\":False,    ## Use 3 channels (RGB) or 1(Greyscale)\n",
    "           \"save_model\":False,          ## Save model after training\n",
    "           \"save_every\":10,             ## Save model every x episodes\n",
    "           \"n_conv_layers\":2,           ## Number of layers to use (**For now, only 2 supported**)\n",
    "           \"n_channels_out_1\":20,       ## Number of channels/filters in conv1\n",
    "           \"n_channels_out_2\":40,       ## Number of channels/filters in conv1\n",
    "           \"lr\":0.0005,                 ## Learning rate for training\n",
    "           \"batch_size\":1,              ## Update policy ever x episodes\n",
    "           \"n_episodes\":3,              ## Number of episodes to train for\n",
    "           \"gamma\":0.99,                ## Discount factor for reward\n",
    "           \"device\":\"cuda\",             ## Device to train on\n",
    "           \"kernel_size\":3,             ## No support for varying kernel sizes yet\n",
    "           \"render\":False               ## Render gameplay\n",
    "          }\n",
    "\n",
    "options[\"n_channels_in\"] = 3 if options[\"use_all_channels\"] else 1  ## Number of input (color) channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper to turn observations into torch tensor, and read in observation info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(options[\"game\"])\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_dim_x_d2 = observation.shape[0] // 2\n",
    "n_dim_y_d2 = observation.shape[1] // 2\n",
    "if options[\"use_all_channels\"]:\n",
    "    def prepro(I):\n",
    "        \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "        I = I[::2,::2,:] # downsample by factor of 2\n",
    "        return torch.tensor(I.reshape(1, options[\"n_channels_in\"], n_dim_x_d2, n_dim_y_d2)).float()\n",
    "else:\n",
    "    def prepro(I):\n",
    "        \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "        I = I[::2,::2,0] # downsample by factor of 2\n",
    "        return torch.tensor(I.reshape(1, 1, n_dim_x_d2, n_dim_y_d2)).float()\n",
    "    \n",
    "n_dim_x = prepro(observation).shape[2]\n",
    "n_dim_y = prepro(observation).shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Base: Conv1 -> Conv2 -> FC -> Softmax -> Action Probabilities\n",
    "class Base_Policy_2(nn.Module):\n",
    "    def __init__(self, n_out_channels_1=options[\"n_channels_out_1\"], \n",
    "                       n_out_channels_2=options[\"n_channels_out_2\"]):\n",
    "        \n",
    "        super(Base_Policy_2, self).__init__()\n",
    "        self.kernel_size = options[\"kernel_size\"]\n",
    "        self.conv1 = nn.Conv2d(options[\"n_channels_in\"], n_out_channels_1, kernel_size=self.kernel_size )\n",
    "        self.conv2 = nn.Conv2d(n_out_channels_1, n_out_channels_2, kernel_size=self.kernel_size )\n",
    "        self.fc = nn.Linear(n_out_channels_2 * (n_dim_x - 2*(self.kernel_size-1)) *(n_dim_y - 2*(self.kernel_size-1)), \n",
    "                            env.action_space.n, bias=False) ## Assumes kernel_size=3, Can make general later\n",
    "        \n",
    "        ## Training\n",
    "        #self.policy_history = Variable(torch.Tensor()).to(device=options[\"device\"]) \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.reward_history = []         # Overall reward and loss history\n",
    "        self.loss_history = []\n",
    "\n",
    "    def forward(self, x):    \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.fc(x.view(-1))\n",
    "        return torch.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Base: Conv1 -> Conv2 -> FC -> Softmax -> Action Probabilities\n",
    "class Reuse_Policy_2(nn.Module):\n",
    "    def __init__(self, n_out_channels_1=options[\"n_channels_out_1\"], \n",
    "                       n_out_channels_2=options[\"n_channels_out_2\"],\n",
    "                       n_channels_in=options[\"n_channels_in\"],\n",
    "                       kernel_size=options[\"kernel_size\"]):\n",
    "        super(Reuse_Policy_2, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.x_prev = None\n",
    "        \n",
    "        ## Layers\n",
    "        self.conv1 = nn.Conv2d(n_channels_in, n_out_channels_1, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv2d(n_out_channels_1, n_out_channels_2, kernel_size=kernel_size)\n",
    "        self.fc = nn.Linear(n_out_channels_2 * (n_dim_x - 2*(kernel_size-1)) *(n_dim_y - 2*(kernel_size-1)), \n",
    "                            env.action_space.n, \n",
    "                            bias=False) ## Assumes kernel_size=3\n",
    "        \n",
    "        self.last_c1 = None  ## Last activation of conv1 layer\n",
    "        self.last_c2 = None  ## Last activation of conv2 layer\n",
    "        \n",
    "        ## Training\n",
    "        #self.policy_history = Variable(torch.Tensor()).to(device=options[\"device\"]) \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.reward_history = []         # Overall reward and loss history\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## Get difference of frames\n",
    "        if self.x_prev is not None:\n",
    "            x_diff = (self.x_prev - x)\n",
    "            self.x_prev = x\n",
    "            \n",
    "        ## Process for the first time\n",
    "        else:\n",
    "            out = F.relu(self.conv1(x))\n",
    "            self.last_c1= out.clone()\n",
    "            out = F.relu(self.conv2(out))\n",
    "            self.last_c2= out.clone()\n",
    "            out = out.view(-1)\n",
    "            out = self.fc(out)\n",
    "            out = torch.softmax(out, dim=-1)\n",
    "            self.x_prev = x\n",
    "            return out\n",
    "            \n",
    "        ## Get indices to redo\n",
    "        redo_idx = x_diff.nonzero()\n",
    "        if redo_idx.nelement() == 0:\n",
    "            out = self.last_c2.view(-1)\n",
    "            out = self.fc(out)\n",
    "            return torch.softmax(out, dim=-1)\n",
    "\n",
    "        ## Get min/max of indices to redo, accounting for size of convolution and borders\n",
    "        min_idx_x = redo_idx.min(-2)[0][2].item()\n",
    "        min_idx_y = redo_idx.min(-2)[0][3].item()\n",
    "        max_idx_x = redo_idx.max(-2)[0][2].item()\n",
    "        max_idx_y = redo_idx.max(-2)[0][3].item()\n",
    "        r_x1 = max(min_idx_x - (self.kernel_size - 1), 0)\n",
    "        r_x2 = min(max_idx_x + self.kernel_size, n_dim_x)\n",
    "        r_y1 = max(min_idx_y - (self.kernel_size - 1), 0)\n",
    "        r_y2 = min(max_idx_y + self.kernel_size, n_dim_y)\n",
    "        \n",
    "        ## Redo first level of convolutions, assign result to the proper area of the previous activations\n",
    "        redo_area = x[:,:,r_x1:r_x2,r_y1:r_y2]\n",
    "        redo_area = self.conv1(redo_area)\n",
    "        redo_area = F.relu(redo_area)\n",
    "        c1 = self.last_c1.clone()\n",
    "        c1[:,:,r_x1:r_x1+redo_area.shape[2],r_y1:r_y1+redo_area.shape[3]] = redo_area\n",
    "        \n",
    "        ## Get min/max of indices to redo for 2nd conv, accounting for size of convolution and borders\n",
    "        r_x1 = max(r_x1 - (self.kernel_size - 1), 0)\n",
    "        r_x2 = min(r_x2 + self.kernel_size, n_dim_x)\n",
    "        r_y1 = max(r_y1 - (self.kernel_size - 1), 0)\n",
    "        r_y2 = min(r_y2 + self.kernel_size, n_dim_y)\n",
    "        \n",
    "        ## Redo 2nd layer of convolutions, assign it to the proper area of the previous activations\n",
    "        redo_area = F.relu(self.conv2(c1[:,:,r_x1:r_x2,r_y1:r_y2]))\n",
    "        c2 = self.last_c2.clone()\n",
    "        c2[:,:,r_x1:r_x1+redo_area.shape[2],r_y1:r_y1+redo_area.shape[3]] = redo_area\n",
    "        \n",
    "        out = c2.view(-1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        self.last_c1 = c1\n",
    "        self.last_c2 = c2\n",
    "        \n",
    "        return torch.softmax(out, dim=-1)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = options[\"device\"]\n",
    "\n",
    "def select_action(state, policy):\n",
    "    state = state.to(device=device)\n",
    "    #state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action).to(device=device))\n",
    "    return action.item()\n",
    "\n",
    "def update_policy(policy, optimizer, retain_graph=False):\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + options[\"gamma\"] * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns).to(device=device)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 0.0001)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = sum(policy_loss)\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference - Base Policy v.s. Reuse Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 592379725]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward for this ep(0): 2070.00\n",
      "Total reward for this ep(1): 2277.00\n",
      "Total reward for this ep(2): 2346.00\n",
      "8.512325286865234\n"
     ]
    }
   ],
   "source": [
    "policy = Base_Policy_2()\n",
    "policy = policy.to(device=options[\"device\"])\n",
    "optimizer = optim.SGD(policy.parameters(), lr=options[\"lr\"], momentum=1e-3)\n",
    "\n",
    "batch_size = options[\"batch_size\"]\n",
    "observation = env.reset()\n",
    "\n",
    "steps=0\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "start = time.time()\n",
    "\n",
    "while(episode_number < options[\"n_episodes\"]):\n",
    "    # preprocess the observation\n",
    "    curr_img = prepro(observation)\n",
    "    x = curr_img.reshape(1,options[\"n_channels_in\"],n_dim_x_d2,n_dim_y_d2)\n",
    "    \n",
    "    # forward the policy network and sample an action from the returned probability\n",
    "    with torch.no_grad():\n",
    "        action = select_action(torch.tensor(x).float(), policy)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        steps += 1\n",
    "\n",
    "        policy.rewards.append(reward)\n",
    "        reward_sum += reward\n",
    "\n",
    "        if done: # an episode finished\n",
    "            print(\"Total reward for this ep({0:d}): {1:.2f}\".format(episode_number, reward_sum))\n",
    "            episode_number += 1\n",
    "            reward_sum = 0  \n",
    "            observation = env.reset() # reset env\n",
    "        \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward for this ep(0): 1818.00\n",
      "Total reward for this ep(1): 3091.00\n",
      "Total reward for this ep(2): 3160.00\n",
      "4.004498243331909\n"
     ]
    }
   ],
   "source": [
    "policy = Reuse_Policy_2()\n",
    "policy = policy.to(device=options[\"device\"])\n",
    "optimizer = optim.SGD(policy.parameters(), lr=options[\"lr\"], momentum=1e-3)\n",
    "\n",
    "batch_size = options[\"batch_size\"]\n",
    "observation = env.reset()\n",
    "\n",
    "steps=0\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "start = time.time()\n",
    "\n",
    "while(episode_number < options[\"n_episodes\"]):\n",
    "    # preprocess the observation\n",
    "    curr_img = prepro(observation)\n",
    "    #x = curr_img.reshape(1,1,n_dim_x,n_dim_y)\n",
    "    x = curr_img.reshape(1,options[\"n_channels_in\"],n_dim_x_d2,n_dim_y_d2)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # forward the policy network and sample an action from the returned probability\n",
    "        action = select_action(torch.tensor(x).float(), policy)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        steps += 1\n",
    "\n",
    "        policy.rewards.append(reward)\n",
    "        reward_sum += reward\n",
    "\n",
    "        if done: # an episode finished\n",
    "            print(\"Total reward for this ep({0:d}): {1:.2f}\".format(episode_number, reward_sum))\n",
    "            episode_number += 1\n",
    "            reward_sum = 0  \n",
    "            observation = env.reset() # reset env\n",
    "        \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward for this ep(0): 3105.00\n",
      "Total reward for this ep(1): 2415.00\n",
      "Total reward for this ep(2): 3450.00\n",
      "9.63712739944458\n"
     ]
    }
   ],
   "source": [
    "policy = Base_Policy_2()\n",
    "policy = policy.to(device=options[\"device\"])\n",
    "optimizer = optim.Adam(policy.parameters(), lr=options[\"lr\"])\n",
    "\n",
    "episode_number = 0\n",
    "reward_sum = 0\n",
    "observation = env.reset()\n",
    "batch_size = options[\"batch_size\"]\n",
    "x_prev = torch.zeros((1,options[\"n_channels_in\"],n_dim_x_d2,n_dim_y_d2))\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "while(episode_number < options[\"n_episodes\"]):    \n",
    "    curr_img = prepro(observation)\n",
    "    x = curr_img.reshape(1,1,n_dim_x_d2,n_dim_y_d2)\n",
    "    \n",
    "    # forward the policy network and sample an action from the returned probability\n",
    "    action = select_action(torch.tensor(x - x_prev).float(), policy)\n",
    "    \n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    policy.rewards.append(reward)\n",
    "    reward_sum += reward\n",
    "    x_prev = x\n",
    "    \n",
    "    if done: # an episode finished\n",
    "        print(\"Total reward for this ep({0:d}): {1:.2f}\".format(episode_number, reward_sum))\n",
    "        reward_sum=0\n",
    "        episode_number += 1\n",
    "\n",
    "        if episode_number % batch_size == 0:\n",
    "            update_policy(policy, optimizer, retain_graph=False)\n",
    "        \n",
    "        if options[\"save_model\"] and episode_number % options[\"save_every\"] == 0:\n",
    "            PATH = 'models/reuse_fix'\n",
    "            torch.save({\n",
    "                'episode_number': episode_number,\n",
    "                'model_state_dict': policy.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, PATH)\n",
    "            \n",
    "        observation = env.reset()\n",
    "        reward_sum=0\n",
    "        \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward for this ep(0): 4071.00\n",
      "Total reward for this ep(1): 3381.00\n",
      "Total reward for this ep(2): 3519.00\n",
      "15.549543619155884\n"
     ]
    }
   ],
   "source": [
    "policy = Reuse_Policy_2()\n",
    "policy = policy.to(device=options[\"device\"])\n",
    "optimizer = optim.Adam(policy.parameters(), lr=options[\"lr\"])\n",
    "\n",
    "episode_number = 0\n",
    "reward_sum=0\n",
    "observation = env.reset()\n",
    "policy.x_prev=None\n",
    "\n",
    "batch_size = options[\"batch_size\"]\n",
    "x_prev = torch.zeros((1,1,n_dim_x_d2,n_dim_y_d2))\n",
    "\n",
    "start=time.time()\n",
    "while(episode_number < options[\"n_episodes\"]):\n",
    "    curr_img = prepro(observation)\n",
    "    x = curr_img.reshape(1,options[\"n_channels_in\"],n_dim_x_d2,n_dim_y_d2)\n",
    "    \n",
    "    # forward the policy network and sample an action from the returned probability\n",
    "    action = select_action(torch.tensor(x - x_prev).float(), policy)\n",
    "    \n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    policy.rewards.append(reward)\n",
    "    reward_sum += reward\n",
    "    x_prev = x\n",
    "    \n",
    "    if done: # an episode finished\n",
    "        print(\"Total reward for this ep({0:d}): {1:.2f}\".format(episode_number, reward_sum))\n",
    "        episode_number += 1\n",
    "\n",
    "        if episode_number % batch_size == 0:\n",
    "            update_policy(policy, optimizer, retain_graph=False)\n",
    "            policy.x_prev=None\n",
    "            \n",
    "            ## Need to use if batch size isn't 1\n",
    "            #if episode_number % options[\"n_reset_graph\"] != 0 :\n",
    "            #    update_policy(policy, optimizer, retain_graph=True)\n",
    "            #else:\n",
    "            #    update_policy(policy, optimizer, retain_graph=False)\n",
    "            #    policy.x_prev=None\n",
    "        \n",
    "        ## Save model\n",
    "        if options[\"save_model\"] and episode_number % options[\"save_every\"] == 0:\n",
    "            PATH = 'models/reuse_fix'\n",
    "            torch.save({\n",
    "                'episode_number': episode_number,\n",
    "                'model_state_dict': policy.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, PATH)\n",
    "            \n",
    "        observation = env.reset()\n",
    "        reward_sum=0\n",
    "        \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
